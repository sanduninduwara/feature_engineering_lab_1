# -*- coding: utf-8 -*-
"""190420V_Feature_Engineering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gTstSlgvBxjupnHt38oSvzQAq_5UAHQM
"""

# from google.colab import drive
# drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.feature_selection import SelectKBest, f_classif
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
import matplotlib.pyplot as plt
from imblearn.over_sampling import RandomOverSampler
LE=LabelEncoder()

# Load the dataset from CSV file
train = pd.read_csv('/content/drive/MyDrive/Sem7/ML/feature_engineering/train.csv')
valid = pd.read_csv('/content/drive/MyDrive/Sem7/ML/feature_engineering/valid.csv')
test = pd.read_csv('/content/drive/MyDrive/Sem7/ML/feature_engineering/test.csv')

train_original=train.copy()
valid_original=valid.copy()
test_original=test.copy()

train.shape

train_1=train.dropna(subset=[train.columns[256]])
train_2=train.dropna(subset=[train.columns[257]])
train_3=train.dropna(subset=[train.columns[258]])
train_4=train.dropna(subset=[train.columns[259]])

train_1.shape, train_2.shape, train_3.shape,

valid_1=valid.dropna(subset=[valid.columns[256]])
valid_2=valid.dropna(subset=[valid.columns[257]])
valid_3=valid.dropna(subset=[valid.columns[258]])
valid_4=valid.dropna(subset=[valid.columns[259]])

# test_1=test.dropna(subset=[test.columns[256]])
# test_2=test.dropna(subset=[test.columns[257]])
# test_3=test.dropna(subset=[test.columns[258]])
# test_4=test.dropna(subset=[test.columns[259]])

train.head()

Y=[]
lable="y"

X_train_1 = train_1.iloc[:, :256]
X_train_2 = train_2.iloc[:, :256]
X_train_3 = train_3.iloc[:, :256]
X_train_4 = train_4.iloc[:, :256]

Y_train_1 = train_1.iloc[:, 256].to_frame(lable)
Y_train_2 = train_2.iloc[:, 257].to_frame(lable)
Y_train_3 = train_3.iloc[:, 258].to_frame(lable)
Y_train_4 = train_4.iloc[:, 259].to_frame(lable)

Y.append(Y_train_1)
Y.append(Y_train_2)
Y.append(Y_train_3)
Y.append(Y_train_4)

X_train_1.shape, X_train_2.shape, X_train_3.shape, X_train_4.shape

Y_train_1.shape, Y_train_2.shape, Y_train_3.shape, Y_train_4.shape

X_valid_1 = valid_1.iloc[:, :256]
X_valid_2 = valid_2.iloc[:, :256]
X_valid_3 = valid_3.iloc[:, :256]
X_valid_4 = valid_4.iloc[:, :256]

Y_valid_1 = valid_1.iloc[:, 256].to_frame(lable)
Y_valid_2 = valid_2.iloc[:, 257].to_frame(lable)
Y_valid_3 = valid_3.iloc[:, 258].to_frame(lable)
Y_valid_4 = valid_4.iloc[:, 259].to_frame(lable)

X_valid_2.shape, Y_valid_2.shape

X_test = test.iloc[:, :256]

for i in range(4):
  unique_classes, class_counts = np.unique(Y[i], return_counts=True)
  plt.bar(unique_classes, class_counts)
  plt.xlabel(f"Label {i+1}")
  plt.ylabel('Number of samples')
  plt.title('Label Distribution')
  plt.show()

sampler = RandomOverSampler(random_state=45)
k = sampler.fit_resample(X_train_4, Y_train_4[lable])
X_train_4, Y_train_4= k[0],k[1].to_frame(name=lable)
X_train_4.shape

unique_classes, class_counts = np.unique(Y_train_4, return_counts=True)
plt.bar(unique_classes, class_counts)
plt.xlabel(f"Label {i+1}")
plt.ylabel('Number of samples')
plt.title('Label Distribution')
plt.show()

#XGBoost Classifier
def xgBoostModel(X_train,Y_train):
  num_classes = len(Y_train[lable].unique())
  if num_classes == 2:
    objective = 'binary:logistic'
  else:
    objective = 'multi:softmax'
  # Create an XGBoost model
  model = xgb.XGBClassifier(objective=objective, random_state=39, tree_method='gpu_hist')
  Y_train_encoded = LE.fit_transform(Y_train[lable])
  # Train the model
  model.fit(X_train, Y_train_encoded)
  return model

#Support Vector Classifier
def svmModel(X_train,Y_train):
  # Create an Support Vector Classifier
  model = SVC(kernel='rbf', decision_function_shape='ovr', random_state=42)
  Y_train_encoded = LE.fit_transform(Y_train[lable])
  # Train the model
  model.fit(X_train, Y_train_encoded)
  return model

def evaluator(X_train,Y_train,X_valid,Y_valid,X_test,model,y_lable="1" ):
  model_name = model.__class__.__name__
  print(model_name)

  Y_pred_encoded = model.predict(X_valid)
  Y_pred = LE.inverse_transform(Y_pred_encoded)

  # Evaluate the y1 using valid data
  accuracy = accuracy_score(Y_valid, Y_pred)
  print(f"Valid Data Accuracy for y{y_lable}: {accuracy:.2f}")

  # Test data the model using test data
  Y_pred_test_encoded = model.predict(X_test)
  Y_pred_test = LE.inverse_transform(Y_pred_test_encoded)

  return Y_pred_test

#create the svm model for y1
model_svm_1=svmModel(X_train_1,Y_train_1)
y_pred_1=evaluator(X_train_1,Y_train_1,X_valid_1,Y_valid_1,X_test,model_svm_1)

#create the XGBoost model for y1
model_XGBoost_1=xgBoostModel(X_train_1,Y_train_1)
y_pred_low=evaluator(X_train_1,Y_train_1,X_valid_1,Y_valid_1,X_test,model_XGBoost_1)

# Create the SVM model for y2
model_svm_2 = svmModel(X_train_2, Y_train_2)
y_pred_2=evaluator(X_train_2, Y_train_2, X_valid_2, Y_valid_2, X_test, model_svm_2)

# Create the XGBoost model for y2
model_XGBoost_2 = xgBoostModel(X_train_2, Y_train_2)
y_pred_low=evaluator(X_train_2, Y_train_2, X_valid_2, Y_valid_2, X_test, model_XGBoost_2)

# Create the SVM model for y3
model_svm_3 = svmModel(X_train_3, Y_train_3)
y_pred_3=evaluator(X_train_3, Y_train_3, X_valid_3, Y_valid_3, X_test, model_svm_3)

# Create the XGBoost model for y3
model_XGBoost_3 = xgBoostModel(X_train_3, Y_train_3)
y_pred_low=evaluator(X_train_3, Y_train_3, X_valid_3, Y_valid_3, X_test, model_XGBoost_3)

# Create the SVM model for y4
model_svm_4 = svmModel(X_train_4, Y_train_4)
y_pred_4=evaluator(X_train_4, Y_train_4, X_valid_4, Y_valid_4, X_test, model_svm_4)

# Create the XGBoost model for y4
model_XGBoost_4 = xgBoostModel(X_train_4, Y_train_4)
y_pred_low=evaluator(X_train_4, Y_train_4, X_valid_4, Y_valid_4, X_test, model_XGBoost_4)

"""**Feature Selection**

Using corelation matrix
"""

import pandas as pd
import numpy as np

# Adding y to the training data set
new_X_train=X_train_1.assign(y=Y_train_1)

# Assuming your dataset is stored in a DataFrame called 'data'
correlation_matrix = new_X_train.corr()
correlation_matrix

# Find highly correlated features using the mask
correlated_features = set()
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > 0.5:
            correlated_feature= correlation_matrix.columns[i] if correlation_matrix.iloc[i,-1] <= correlation_matrix.iloc[j,-1] else correlation_matrix.columns[j]
            correlated_features.add(correlated_feature)

# Remove the highly correlated features from your dataset
X_train_filtered = X_train_1.drop(columns=correlated_features)
X_valid_filtered = X_valid_1.drop(columns=correlated_features)
X_test_filtered = X_test.drop(columns=correlated_features)

X_train_filtered.shape

#Make predictions
model=svmModel(X_train_filtered,Y_train_1)
Y_pred_encoded = model.predict(X_valid_filtered)
Y_pred = LE.inverse_transform(Y_pred_encoded)

# Evaluate the model using valid data
accuracy = accuracy_score(Y_valid_1, Y_pred)
print(f"Accuracy: {accuracy:.2f}")

"""Principal Component Analysis"""

from sklearn.decomposition import PCA

def pca(X_train,X_valid,X_test,desired_variance_ratio=0.95):
  desired_variance_ratio = desired_variance_ratio
  pca = PCA(n_components=desired_variance_ratio, svd_solver='full')

  X_train_pca = pca.fit_transform(X_train)
  X_valid_pca = pca.transform(X_valid)
  X_test_pca = pca.transform(X_test)

  return X_train_pca ,X_valid_pca ,X_test_pca

X_train_pca_1 ,X_valid_pca_1 ,X_test_pca_1= pca(X_train_1,X_valid_1,X_test)
X_train_pca_1.shape,X_valid_pca_1.shape,X_test_pca_1.shape

# Create the SVM model with PCA for y1
pca_model_svm_1=svmModel(X_train_pca_1,Y_train_1)
y_pred_after_1=evaluator(X_train_pca_1,Y_train_1,X_valid_pca_1,Y_valid_1,X_test_pca_1,pca_model_svm_1,"1")

# Create the XGBoost model with PCA for y2
pca_model_XGBoost_1=xgBoostModel(X_train_pca_1,Y_train_1)
y_pred_after_low=evaluator(X_train_pca_1,Y_train_1,X_valid_pca_1,Y_valid_1,X_test_pca_1,pca_model_XGBoost_1,"1")

X_train_pca_1.shape,X_valid_pca_1.shape,X_test_pca_1.shape

# Perform PCA on X_train_pca_2, X_valid_pca_2, and X_test_pca_2
X_train_pca_2, X_valid_pca_2, X_test_pca_2 = pca(X_train_2, X_valid_2, X_test)
X_train_pca_2.shape, X_valid_pca_2.shape, X_test_pca_2.shape

# Create the SVM model with PCA for y2
pca_model_svm_2 = svmModel(X_train_pca_2, Y_train_2)
y_pred_after_2=evaluator(X_train_pca_2, Y_train_2, X_valid_pca_2, Y_valid_2, X_test_pca_2, pca_model_svm_2, "2")

# Create the XGBoost model with PCA for y2
pca_model_XGBoost_2 = xgBoostModel(X_train_pca_2, Y_train_2)
y_pred_after_low=evaluator(X_train_pca_2, Y_train_2, X_valid_pca_2, Y_valid_2, X_test_pca_2, pca_model_XGBoost_2, "2")

# Perform PCA on X_train_pca_3, X_valid_pca_3, and X_test_pca_3
X_train_pca_3, X_valid_pca_3, X_test_pca_3 = pca(X_train_3, X_valid_3, X_test)
X_train_pca_3.shape, X_valid_pca_3.shape, X_test_pca_3.shape

# Create the SVM model with PCA for y3
pca_model_svm_3 = svmModel(X_train_pca_3, Y_train_3)
y_pred_after_3=evaluator(X_train_pca_3, Y_train_3, X_valid_pca_3, Y_valid_3, X_test_pca_3, pca_model_svm_3, "3")

# Create the XGBoost model with PCA for y3
pca_model_XGBoost_3 = xgBoostModel(X_train_pca_3, Y_train_3)
y_pred_after_low=evaluator(X_train_pca_3, Y_train_3, X_valid_pca_3, Y_valid_3, X_test_pca_3, pca_model_XGBoost_3, "3")

# Perform PCA on X_train_pca_4, X_valid_pca_4, and X_test_pca_4
X_train_pca_4, X_valid_pca_4, X_test_pca_4 = pca(X_train_4, X_valid_4, X_test)
X_train_pca_4.shape, X_valid_pca_4.shape, X_test_pca_4.shape

# Create the SVM model with PCA for y4
pca_model_svm_4 = svmModel(X_train_pca_4, Y_train_4)
y_pred_after_4=evaluator(X_train_pca_4, Y_train_4, X_valid_pca_4, Y_valid_4, X_test_pca_4, pca_model_svm_4, "4")

# Create the XGBoost model with PCA for y4
pca_model_XGBoost_4 = xgBoostModel(X_train_pca_4, Y_train_4)
y_pred_after_low=evaluator(X_train_pca_4, Y_train_4, X_valid_pca_4, Y_valid_4, X_test_pca_4, pca_model_XGBoost_4, "4")

"""Generate CSVs

"""

def createCSVOutput(y_pred, y_pred_after, new_features, label):
  data = {
    'Predicted labels before feature engineering': y_pred,
    'Predicted labels after feature engineering': y_pred_after,
    'No of new features': [new_features.shape[1]] * len(y_pred),
  }
  for i in range(new_features.shape[1]):
    data[f'new_feature_{i+1}'] = new_features[:, i]
  for i in range(new_features.shape[1], 256):
        data[f'new_feature_{i+1}'] = [0] * len(y_pred)
  df = pd.DataFrame(data)
  filename = f'/content/drive/MyDrive/Sem7/ML/feature_engineering/190420V_label_{label}.csv'
  df.to_csv(filename, index=False)

createCSVOutput(y_pred_1, y_pred_after_1, X_test_pca_1, 1)

createCSVOutput(y_pred_2, y_pred_after_2, X_test_pca_2, 2)

createCSVOutput(y_pred_3, y_pred_after_3, X_test_pca_3, 3)

createCSVOutput(y_pred_4, y_pred_after_4, X_test_pca_4, 4)